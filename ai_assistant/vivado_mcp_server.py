"""
Vivado FPGA Expert - MCP (Model Context Protocol) Server
VS Code chat participant i√ßin backend API
"""

import os
import json
import sys
from pathlib import Path
from dotenv import load_dotenv
from openai import OpenAI
import chromadb
from chromadb.config import Settings
from flask import Flask, request, jsonify
from flask_cors import CORS

# ============================================================================
# BA≈ûLATMA
# ============================================================================

load_dotenv()
app = Flask(__name__)
CORS(app)

# OpenAI ve ChromaDB ba≈ülat
api_key = os.getenv('OPENAI_API_KEY')
if not api_key:
    print("‚ùå OPENAI_API_KEY bulunamadƒ±!", file=sys.stderr)
    sys.exit(1)

client = OpenAI(api_key=api_key)

# ChromaDB baƒülantƒ±sƒ±
db_path = Path(__file__).parent / "vivado_vectordb_full"
chroma_client = chromadb.PersistentClient(
    path=str(db_path),
    settings=Settings(anonymized_telemetry=False)
)

try:
    collection = chroma_client.get_collection("vivado_full_system")
    print(f"‚úÖ RAG Collection y√ºklendi: {collection.count()} dok√ºman")
except Exception as e:
    print(f"‚ùå ChromaDB collection y√ºklenemedi: {e}", file=sys.stderr)
    sys.exit(1)

# ============================================================================
# YARDIMCI FONKSƒ∞YONLAR
# ============================================================================

def search_rag(query: str, n_results: int = 5) -> list:
    """RAG sisteminden ilgili d√∂k√ºmanlarƒ± ara"""
    try:
        results = collection.query(
            query_texts=[query],
            n_results=n_results,
            include=['documents', 'metadatas', 'distances']
        )
        
        contexts = []
        for i, doc in enumerate(results['documents'][0]):
            metadata = results['metadatas'][0][i]
            distance = results['distances'][0][i]
            
            contexts.append({
                'content': doc,
                'source': metadata.get('source', 'unknown'),
                'file': metadata.get('file', 'unknown'),
                'relevance': 1.0 - distance
            })
        
        return contexts
    except Exception as e:
        print(f"‚ùå RAG arama hatasƒ±: {e}", file=sys.stderr)
        return []

def create_prompt(query: str, contexts: list, command: str = None) -> str:
    """GPT i√ßin prompt olu≈ütur"""
    
    system_prompt = """Sen Xilinx Vivado Design Suite konusunda uzman bir FPGA m√ºhendisisin.
    
Uzmanlƒ±k alanlarƒ±n:
- Vivado IP Integrator (Block Design)
- AXI4/AXI4-Lite/AXI-Stream protokolleri
- Zynq-7000 ve UltraScale+ SoC
- TCL scripting (create_bd, add IP, connect)
- Verilog, VHDL, SystemVerilog
- Vitis HLS (High-Level Synthesis)
- DDR4, PCIe, Ethernet IP'leri
- Timing constraints (XDC)
- Implementation, synthesis

Yanƒ±tlarƒ±nda:
‚úÖ Net ve pratik bilgi ver
‚úÖ Kod √∂rnekleri ekle (Verilog/VHDL/TCL)
‚úÖ UG/PG referanslarƒ± g√∂ster
‚úÖ Best practice'leri belirt
‚ùå Gereksiz detaya girme
‚ùå Sadece teori anlatma"""

    # Context'leri formatla
    context_text = "\n\n".join([
        f"üìÑ **Kaynak:** {ctx['file']}\n```\n{ctx['content'][:500]}...\n```"
        for ctx in contexts[:3]
    ])
    
    # Komuta g√∂re prompt ayarla
    if command == 'code':
        user_prompt = f"""**G√∂rev:** Kod √∂rneƒüi olu≈ütur

**Soru:** {query}

**ƒ∞lgili D√∂k√ºmanlar:**
{context_text}

**Beklenen:** √áalƒ±≈üan kod √∂rneƒüi (Verilog/VHDL/TCL) + a√ßƒ±klama"""
    
    elif command == 'explain':
        user_prompt = f"""**G√∂rev:** Kavramƒ± a√ßƒ±kla

**Soru:** {query}

**ƒ∞lgili D√∂k√ºmanlar:**
{context_text}

**Beklenen:** Kƒ±sa a√ßƒ±klama + kullanƒ±m √∂rneƒüi + UG/PG referans"""
    
    elif command == 'search':
        user_prompt = f"""**G√∂rev:** D√∂k√ºman ara

**Soru:** {query}

**ƒ∞lgili D√∂k√ºmanlar:**
{context_text}

**Beklenen:** ƒ∞lgili UG/PG'leri listele + kƒ±sa √∂zet"""
    
    else:
        user_prompt = f"""**Soru:** {query}

**ƒ∞lgili D√∂k√ºmanlar:**
{context_text}

**Yanƒ±t ver:** Soru baƒülamƒ±nda en uygun yanƒ±t"""
    
    return system_prompt, user_prompt

def query_gpt(query: str, contexts: list, command: str = None) -> dict:
    """GPT'ye sor ve yanƒ±tƒ± al"""
    try:
        system_prompt, user_prompt = create_prompt(query, contexts, command)
        
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.7,
            max_tokens=1500
        )
        
        answer = response.choices[0].message.content
        
        return {
            'success': True,
            'answer': answer,
            'contexts': contexts,
            'tokens': response.usage.total_tokens
        }
        
    except Exception as e:
        return {
            'success': False,
            'error': str(e)
        }

# ============================================================================
# REST API ENDPOINTS
# ============================================================================

@app.route('/health', methods=['GET'])
def health_check():
    """Server saƒülƒ±k kontrol√º"""
    return jsonify({
        'status': 'healthy',
        'collection_size': collection.count(),
        'model': 'gpt-4'
    })

@app.route('/query', methods=['POST'])
def query_endpoint():
    """Ana sorgu endpoint'i"""
    try:
        data = request.json
        query = data.get('query', '')
        command = data.get('command', None)
        n_results = data.get('n_results', 5)
        
        if not query:
            return jsonify({'error': 'Query bo≈ü olamaz'}), 400
        
        # RAG arama
        contexts = search_rag(query, n_results)
        
        # GPT'ye sor
        result = query_gpt(query, contexts, command)
        
        return jsonify(result)
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/search', methods=['POST'])
def search_endpoint():
    """Sadece RAG arama (GPT'siz)"""
    try:
        data = request.json
        query = data.get('query', '')
        n_results = data.get('n_results', 10)
        
        contexts = search_rag(query, n_results)
        
        return jsonify({
            'success': True,
            'results': contexts
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/stats', methods=['GET'])
def stats_endpoint():
    """ƒ∞statistikler"""
    try:
        # Collection metadata
        results = collection.get(include=['metadatas'])
        
        # Kaynak daƒüƒ±lƒ±mƒ±
        sources = {}
        for metadata in results['metadatas']:
            source = metadata.get('source', 'unknown')
            sources[source] = sources.get(source, 0) + 1
        
        return jsonify({
            'total_documents': len(results['ids']),
            'sources': sources,
            'database_path': str(db_path)
        })
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

# ============================================================================
# MAIN
# ============================================================================

if __name__ == '__main__':
    print("=" * 80)
    print("üöÄ Vivado FPGA Expert - MCP Server")
    print("=" * 80)
    print(f"üìö RAG Database: {collection.count()} dok√ºman")
    print(f"ü§ñ Model: GPT-4")
    print(f"üåê Server: http://localhost:5000")
    print("=" * 80)
    print()
    print("‚úÖ Server hazƒ±r! VS Code'dan @vivado ile kullanabilirsiniz.")
    print()
    
    # Flask server ba≈ülat
    app.run(host='127.0.0.1', port=5000, debug=False)
