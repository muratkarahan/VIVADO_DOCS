"""
Vivado FPGA Expert - Tam RAG EÄŸitim Sistemi
TÃ¼m dokÃ¼manlarÄ± (PDF, MD, TXT) iÅŸleyip vector database'e ekler
"""

import os
from pathlib import Path
import chromadb
from chromadb.utils import embedding_functions
from openai import OpenAI
from dotenv import load_dotenv
from tqdm import tqdm
import tiktoken
import json
from datetime import datetime

# PDF iÅŸleme iÃ§in
try:
    from PyPDF2 import PdfReader
    PDF_AVAILABLE = True
except ImportError:
    print("âš ï¸ PyPDF2 yÃ¼klÃ¼ deÄŸil. PDF desteÄŸi kapalÄ±.")
    PDF_AVAILABLE = False

class VivadoRAGTrainer:
    """Vivado dokÃ¼manlarÄ±nÄ± tam RAG sistemi iÃ§in hazÄ±rlayan sÄ±nÄ±f"""
    
    def __init__(self, 
                 workspace_dir="c:/Users/murat/Documents/GitHub/VIVADO_DOCS",
                 db_path="./vivado_vectordb"):
        load_dotenv()
        self.workspace_dir = Path(workspace_dir)
        self.db_path = db_path
        
        # OpenAI client
        api_key = os.getenv('OPENAI_API_KEY')
        if not api_key:
            raise ValueError("âŒ OPENAI_API_KEY bulunamadÄ±! .env dosyasÄ±nÄ± kontrol edin.")
        
        self.client = OpenAI(api_key=api_key)
        
        # ChromaDB setup
        print(f"ğŸ”§ ChromaDB baÅŸlatÄ±lÄ±yor: {db_path}")
        self.chroma_client = chromadb.PersistentClient(path=db_path)
        
        # OpenAI embedding function
        self.embedding_function = embedding_functions.OpenAIEmbeddingFunction(
            api_key=api_key,
            model_name="text-embedding-ada-002"
        )
        
        # Collection oluÅŸtur veya al
        self.collection_name = "vivado_docs_complete"
        try:
            self.collection = self.chroma_client.get_collection(
                name=self.collection_name,
                embedding_function=self.embedding_function
            )
            print(f"âœ… Mevcut collection bulundu: {self.collection.count()} dÃ¶kÃ¼man")
        except:
            self.collection = self.chroma_client.create_collection(
                name=self.collection_name,
                embedding_function=self.embedding_function,
                metadata={"description": "Xilinx Vivado Complete Documentation (PDF+MD+TXT)"}
            )
            print("ğŸ†• Yeni collection oluÅŸturuldu")
        
        # Tokenizer
        self.tokenizer = tiktoken.get_encoding("cl100k_base")
        
        # Ä°statistikler
        self.stats = {
            'pdf_count': 0,
            'md_count': 0,
            'txt_count': 0,
            'total_chunks': 0,
            'total_tokens': 0,
            'failed_files': [],
            'start_time': datetime.now()
        }
    
    def count_tokens(self, text):
        """Text'in token sayÄ±sÄ±nÄ± hesapla"""
        return len(self.tokenizer.encode(text))
    
    def chunk_text(self, text, chunk_size=1000, overlap=200):
        """Metni chunk'lara bÃ¶l (token-based, akÄ±llÄ± bÃ¶lme)"""
        # Ã–nce paragraflara bÃ¶l
        paragraphs = text.split('\n\n')
        
        chunks = []
        current_chunk = ""
        current_tokens = 0
        
        for para in paragraphs:
            para_tokens = self.count_tokens(para)
            
            # EÄŸer paragraf Ã§ok bÃ¼yÃ¼kse satÄ±rlara bÃ¶l
            if para_tokens > chunk_size:
                lines = para.split('\n')
                for line in lines:
                    line_tokens = self.count_tokens(line)
                    
                    if current_tokens + line_tokens > chunk_size:
                        if current_chunk:
                            chunks.append(current_chunk.strip())
                        current_chunk = line + '\n'
                        current_tokens = line_tokens
                    else:
                        current_chunk += line + '\n'
                        current_tokens += line_tokens
            
            # Normal paragraf
            elif current_tokens + para_tokens > chunk_size:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = para + '\n\n'
                current_tokens = para_tokens
            else:
                current_chunk += para + '\n\n'
                current_tokens += para_tokens
        
        # Son chunk'Ä± ekle
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def process_markdown(self, md_path):
        """Markdown dosyasÄ±nÄ± iÅŸle"""
        try:
            with open(md_path, 'r', encoding='utf-8') as f:
                text = f.read()
            
            if not text.strip():
                return 0
            
            print(f"\nğŸ“ {md_path.name}")
            
            # Chunk'lara bÃ¶l
            chunks = self.chunk_text(text, chunk_size=1000, overlap=150)
            
            if not chunks:
                return 0
            
            all_metadata = []
            for chunk_idx, chunk in enumerate(chunks):
                all_metadata.append({
                    'source': md_path.name,
                    'file_path': str(md_path.relative_to(self.workspace_dir)),
                    'chunk': chunk_idx,
                    'doc_type': self._get_doc_type(md_path),
                    'file_type': 'markdown'
                })
            
            # Unique ID'ler oluÅŸtur
            ids = [f"{md_path.stem}_md_c{m['chunk']}" for m in all_metadata]
            
            # ChromaDB'ye ekle
            self.collection.add(
                documents=chunks,
                metadatas=all_metadata,
                ids=ids
            )
            
            print(f"   âœ… {len(chunks)} chunk eklendi")
            self.stats['md_count'] += 1
            self.stats['total_chunks'] += len(chunks)
            self.stats['total_tokens'] += sum(self.count_tokens(c) for c in chunks)
            
            return len(chunks)
            
        except Exception as e:
            print(f"   âŒ Hata: {e}")
            self.stats['failed_files'].append(str(md_path))
            return 0
    
    def process_pdf(self, pdf_path):
        """PDF dosyasÄ±nÄ± iÅŸle"""
        if not PDF_AVAILABLE:
            print(f"   âš ï¸ PDF desteÄŸi yok, atlanÄ±yor: {pdf_path.name}")
            return 0
            
        try:
            reader = PdfReader(pdf_path)
            total_pages = len(reader.pages)
            
            print(f"\nğŸ“„ {pdf_path.name}")
            print(f"   Sayfa sayÄ±sÄ±: {total_pages}")
            
            all_chunks = []
            all_metadata = []
            
            for page_num in tqdm(range(total_pages), desc="   Sayfa iÅŸleniyor"):
                page = reader.pages[page_num]
                text = page.extract_text()
                
                if not text.strip():
                    continue
                
                # Chunk'lara bÃ¶l
                chunks = self.chunk_text(text, chunk_size=800, overlap=150)
                
                for chunk_idx, chunk in enumerate(chunks):
                    all_chunks.append(chunk)
                    all_metadata.append({
                        'source': pdf_path.name,
                        'file_path': str(pdf_path.relative_to(self.workspace_dir)),
                        'page': page_num + 1,
                        'chunk': chunk_idx,
                        'doc_type': self._get_doc_type(pdf_path),
                        'file_type': 'pdf'
                    })
            
            # ChromaDB'ye ekle
            if all_chunks:
                ids = [f"{pdf_path.stem}_p{m['page']}_c{m['chunk']}" for m in all_metadata]
                
                self.collection.add(
                    documents=all_chunks,
                    metadatas=all_metadata,
                    ids=ids
                )
                
                print(f"   âœ… {len(all_chunks)} chunk eklendi")
                self.stats['pdf_count'] += 1
                self.stats['total_chunks'] += len(all_chunks)
                self.stats['total_tokens'] += sum(self.count_tokens(c) for c in all_chunks)
                
                return len(all_chunks)
            else:
                print(f"   âš ï¸ Metin Ã§Ä±karÄ±lamadÄ±")
                return 0
                
        except Exception as e:
            print(f"   âŒ Hata: {e}")
            self.stats['failed_files'].append(str(pdf_path))
            return 0
    
    def process_txt(self, txt_path):
        """TXT dosyasÄ±nÄ± iÅŸle"""
        try:
            with open(txt_path, 'r', encoding='utf-8') as f:
                text = f.read()
            
            if not text.strip():
                return 0
            
            print(f"\nğŸ“„ {txt_path.name}")
            
            chunks = self.chunk_text(text, chunk_size=1000, overlap=150)
            
            if not chunks:
                return 0
            
            all_metadata = []
            for chunk_idx, chunk in enumerate(chunks):
                all_metadata.append({
                    'source': txt_path.name,
                    'file_path': str(txt_path.relative_to(self.workspace_dir)),
                    'chunk': chunk_idx,
                    'doc_type': self._get_doc_type(txt_path),
                    'file_type': 'text'
                })
            
            ids = [f"{txt_path.stem}_txt_c{m['chunk']}" for m in all_metadata]
            
            self.collection.add(
                documents=chunks,
                metadatas=all_metadata,
                ids=ids
            )
            
            print(f"   âœ… {len(chunks)} chunk eklendi")
            self.stats['txt_count'] += 1
            self.stats['total_chunks'] += len(chunks)
            self.stats['total_tokens'] += sum(self.count_tokens(c) for c in chunks)
            
            return len(chunks)
            
        except Exception as e:
            print(f"   âŒ Hata: {e}")
            self.stats['failed_files'].append(str(txt_path))
            return 0
    
    def _get_doc_type(self, file_path):
        """Dosya tÃ¼rÃ¼nÃ¼ klasÃ¶r yapÄ±sÄ±ndan Ã§Ä±kar"""
        parts = file_path.parts
        
        if 'official_docs' in parts:
            idx = parts.index('official_docs')
            if idx + 1 < len(parts):
                return f"Official/{parts[idx+1]}"
            return "Official/General"
        elif 'code_examples' in parts:
            return "CodeExamples"
        elif 'vivado-examples' in parts:
            return "VivadoExamples"
        elif 'ai_assistant' in parts:
            return "AIAssistant"
        else:
            return "General"
    
    def train_all(self, skip_patterns=None):
        """TÃ¼m dokÃ¼manlarÄ± iÅŸle ve RAG iÃ§in hazÄ±rla"""
        print("\n" + "="*80)
        print("ğŸš€ VIVADO RAG TAM EÄÄ°TÄ°M SÄ°STEMÄ°")
        print("="*80)
        print(f"ğŸ“‚ Workspace: {self.workspace_dir}")
        print(f"ğŸ’¾ Database: {self.db_path}")
        
        if skip_patterns is None:
            skip_patterns = [
                'node_modules',
                '.git',
                '__pycache__',
                'vivado_vectordb',
                '.vscode',
                'vscode-extension'
            ]
        
        # DosyalarÄ± topla
        print("\nğŸ” Dosyalar taranÄ±yor...")
        
        md_files = []
        pdf_files = []
        txt_files = []
        
        for pattern in ['**/*.md', '**/*.pdf', '**/*.txt']:
            for file_path in self.workspace_dir.rglob(pattern.split('/')[-1]):
                # Skip patterns kontrolÃ¼
                if any(skip in str(file_path) for skip in skip_patterns):
                    continue
                
                if file_path.suffix == '.md':
                    md_files.append(file_path)
                elif file_path.suffix == '.pdf':
                    pdf_files.append(file_path)
                elif file_path.suffix == '.txt':
                    txt_files.append(file_path)
        
        total_files = len(md_files) + len(pdf_files) + len(txt_files)
        
        print(f"\nğŸ“Š Bulunan Dosyalar:")
        print(f"   ğŸ“ Markdown: {len(md_files)}")
        print(f"   ğŸ“„ PDF: {len(pdf_files)}")
        print(f"   ğŸ“„ Text: {len(txt_files)}")
        print(f"   ğŸ“š Toplam: {total_files}")
        
        if total_files == 0:
            print("\nâŒ Ä°ÅŸlenecek dosya bulunamadÄ±!")
            return
        
        # Markdown dosyalarÄ±nÄ± iÅŸle
        if md_files:
            print("\n" + "-"*80)
            print("ğŸ“ MARKDOWN DOSYALARI Ä°ÅLENÄ°YOR")
            print("-"*80)
            for md_file in md_files:
                self.process_markdown(md_file)
        
        # PDF dosyalarÄ±nÄ± iÅŸle
        if pdf_files:
            print("\n" + "-"*80)
            print("ğŸ“„ PDF DOSYALARI Ä°ÅLENÄ°YOR")
            print("-"*80)
            for pdf_file in pdf_files:
                self.process_pdf(pdf_file)
        
        # TXT dosyalarÄ±nÄ± iÅŸle
        if txt_files:
            print("\n" + "-"*80)
            print("ğŸ“„ TEXT DOSYALARI Ä°ÅLENÄ°YOR")
            print("-"*80)
            for txt_file in txt_files:
                self.process_txt(txt_file)
        
        # Ä°statistikleri gÃ¶ster
        self._print_final_stats()
        
        # Ä°statistikleri kaydet
        self._save_training_stats()
    
    def _print_final_stats(self):
        """Final istatistiklerini yazdÄ±r"""
        end_time = datetime.now()
        duration = (end_time - self.stats['start_time']).total_seconds()
        
        print("\n" + "="*80)
        print("âœ… RAG EÄÄ°TÄ°MÄ° TAMAMLANDI")
        print("="*80)
        print(f"\nğŸ“Š Ä°statistikler:")
        print(f"   â±ï¸  SÃ¼re: {duration:.1f} saniye")
        print(f"   ğŸ“ Markdown: {self.stats['md_count']} dosya")
        print(f"   ğŸ“„ PDF: {self.stats['pdf_count']} dosya")
        print(f"   ğŸ“„ Text: {self.stats['txt_count']} dosya")
        print(f"   ğŸ“š Toplam Chunk: {self.stats['total_chunks']}")
        print(f"   ğŸ”¢ Toplam Token: {self.stats['total_tokens']:,}")
        print(f"   ğŸ’¾ Database: {self.collection.count()} dÃ¶kÃ¼man")
        
        # Embedding maliyeti (text-embedding-ada-002: $0.0001 / 1K token)
        embedding_cost = (self.stats['total_tokens'] / 1000) * 0.0001
        print(f"   ğŸ’° Tahmini Embedding Maliyet: ${embedding_cost:.4f}")
        
        if self.stats['failed_files']:
            print(f"\nâš ï¸  BaÅŸarÄ±sÄ±z Dosyalar ({len(self.stats['failed_files'])}):")
            for failed in self.stats['failed_files'][:10]:
                print(f"   - {failed}")
            if len(self.stats['failed_files']) > 10:
                print(f"   ... ve {len(self.stats['failed_files']) - 10} dosya daha")
        
        print("="*80 + "\n")
    
    def _save_training_stats(self):
        """EÄŸitim istatistiklerini kaydet"""
        stats_file = Path(self.db_path) / "training_stats.json"
        
        stats_data = {
            **self.stats,
            'start_time': self.stats['start_time'].isoformat(),
            'end_time': datetime.now().isoformat(),
            'collection_name': self.collection_name,
            'total_documents': self.collection.count()
        }
        
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats_data, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“ Ä°statistikler kaydedildi: {stats_file}")
    
    def test_search(self, queries=None):
        """EÄŸitilmiÅŸ RAG sistemini test et"""
        if queries is None:
            queries = [
                "Vivado nedir ve nasÄ±l kullanÄ±lÄ±r?",
                "AXI4-Lite interface nasÄ±l oluÅŸturulur?",
                "Zynq PS-PL haberleÅŸmesi",
                "FPGA synthesis optimization",
                "Vivado TCL scripting"
            ]
        
        print("\n" + "="*80)
        print("ğŸ§ª RAG SÄ°STEMÄ° TEST EDÄ°LÄ°YOR")
        print("="*80)
        
        for query in queries:
            print(f"\nğŸ” Sorgu: '{query}'")
            print("-" * 80)
            
            results = self.collection.query(
                query_texts=[query],
                n_results=3
            )
            
            if results['documents'][0]:
                print("ğŸ“„ SonuÃ§lar:")
                for i, doc in enumerate(results['documents'][0]):
                    metadata = results['metadatas'][0][i]
                    print(f"\n  {i+1}. Kaynak: {metadata['source']}")
                    print(f"     TÃ¼r: {metadata['doc_type']} ({metadata['file_type']})")
                    if 'page' in metadata:
                        print(f"     Sayfa: {metadata['page']}")
                    print(f"     Ä°Ã§erik: {doc[:150]}...")
            else:
                print("   âš ï¸ SonuÃ§ bulunamadÄ±")
        
        print("\n" + "="*80 + "\n")

def main():
    """Ana program"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Vivado FPGA Expert - Tam RAG EÄŸitim Sistemi",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Ã–rnekler:
  # TÃ¼m dokÃ¼manlarÄ± iÅŸle
  python train_rag_complete.py
  
  # Collection'Ä± sÄ±fÄ±rdan oluÅŸtur
  python train_rag_complete.py --reindex
  
  # EÄŸitim sonrasÄ± test yap
  python train_rag_complete.py --test
  
  # Ã–zel workspace klasÃ¶rÃ¼
  python train_rag_complete.py --workspace "D:/MyVivadoDocs"
        """
    )
    
    parser.add_argument('--workspace', 
                       default='c:/Users/murat/Documents/GitHub/VIVADO_DOCS',
                       help='Workspace klasÃ¶rÃ¼')
    parser.add_argument('--db-path', 
                       default='./vivado_vectordb',
                       help='ChromaDB path')
    parser.add_argument('--reindex', 
                       action='store_true',
                       help='Collection silip yeniden indexle')
    parser.add_argument('--test', 
                       action='store_true',
                       help='EÄŸitim sonrasÄ± test yap')
    parser.add_argument('--skip-patterns',
                       nargs='+',
                       help='Atlanacak klasÃ¶r/dosya patternleri')
    
    args = parser.parse_args()
    
    try:
        # Trainer oluÅŸtur
        trainer = VivadoRAGTrainer(args.workspace, args.db_path)
        
        # Reindex ise mevcut collection'Ä± sil
        if args.reindex:
            try:
                trainer.chroma_client.delete_collection(trainer.collection_name)
                print("ğŸ—‘ï¸ Mevcut collection silindi")
                trainer.collection = trainer.chroma_client.create_collection(
                    name=trainer.collection_name,
                    embedding_function=trainer.embedding_function
                )
            except:
                pass
        
        # TÃ¼m dokÃ¼manlarÄ± iÅŸle
        trainer.train_all(skip_patterns=args.skip_patterns)
        
        # Test
        if args.test:
            trainer.test_search()
        
        print("\nâœ¨ Ä°ÅŸlem tamamlandÄ±!")
        print(f"ğŸ¯ Åimdi 'python vivado_agent.py' ile asistanÄ± Ã§alÄ±ÅŸtÄ±rabilirsiniz.")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸ Ä°ÅŸlem kullanÄ±cÄ± tarafÄ±ndan durduruldu.")
    except Exception as e:
        print(f"\nâŒ HATA: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
