"""
Vivado FPGA Expert - TAM SÄ°STEM RAG EÄÄ°TÄ°MÄ°
Bu script bilgisayardaki TÃœM Vivado/FPGA dokÃ¼manlarÄ±nÄ± bulup iÅŸler:
1. Xilinx Kurulum Dizini (C:\Xilinx\) - Vivado, Vitis, Data
2. GitHub FPGA Projeleri (docs_fpga, ax7010, z7lite, z7nano, vb.)
3. VIVADO_DOCS workspace'deki tÃ¼m dokÃ¼manlar
"""

import os
from pathlib import Path
import chromadb
from chromadb.utils import embedding_functions
from openai import OpenAI
from dotenv import load_dotenv
from tqdm import tqdm
import tiktoken
import json
from datetime import datetime
import hashlib

# PDF iÅŸleme
try:
    from PyPDF2 import PdfReader
    PDF_AVAILABLE = True
except ImportError:
    print("âš ï¸ PyPDF2 yÃ¼klÃ¼ deÄŸil. pip install PyPDF2")
    PDF_AVAILABLE = False


class FullSystemRAGTrainer:
    """TÃ¼m sistemdeki Vivado/FPGA dokÃ¼manlarÄ±nÄ± RAG'e ekler"""
    
    # Aranacak dizinler
    SEARCH_PATHS = [
        "C:/Xilinx/2025.1/Vivado",
        "C:/Xilinx/2025.1/Vitis", 
        "C:/Xilinx/2025.1/data",
        "C:/Users/murat/Documents/GitHub/docs_fpga",
        "C:/Users/murat/Documents/GitHub/ax7010_fpga",
        "C:/Users/murat/Documents/GitHub/z7lite_fpga",
        "C:/Users/murat/Documents/GitHub/z7nano_fpga",
        "C:/Users/murat/Documents/GitHub/coraz7_fpga",
        "C:/Users/murat/Documents/GitHub/nexsys_fpga",
        "C:/Users/murat/Documents/GitHub/VIVADO_DOCS",
    ]
    
    def __init__(self, db_path="./vivado_vectordb_full"):
        load_dotenv()
        self.db_path = db_path
        
        # OpenAI setup
        api_key = os.getenv('OPENAI_API_KEY')
        if not api_key:
            raise ValueError("âŒ OPENAI_API_KEY bulunamadÄ±!")
        
        self.client = OpenAI(api_key=api_key)
        
        # ChromaDB setup
        print(f"ğŸ”§ ChromaDB baÅŸlatÄ±lÄ±yor: {db_path}")
        self.chroma_client = chromadb.PersistentClient(path=db_path)
        
        self.embedding_function = embedding_functions.OpenAIEmbeddingFunction(
            api_key=api_key,
            model_name="text-embedding-ada-002"
        )
        
        # Collection
        self.collection_name = "vivado_full_system"
        try:
            self.collection = self.chroma_client.get_collection(
                name=self.collection_name,
                embedding_function=self.embedding_function
            )
            print(f"âœ… Mevcut collection: {self.collection.count()} dÃ¶kÃ¼man")
        except:
            self.collection = self.chroma_client.create_collection(
                name=self.collection_name,
                embedding_function=self.embedding_function,
                metadata={"description": "Full System Vivado/FPGA Documentation"}
            )
            print("ğŸ†• Yeni collection oluÅŸturuldu")
        
        # Tokenizer
        self.tokenizer = tiktoken.get_encoding("cl100k_base")
        
        # Ä°statistikler
        self.stats = {
            'pdf_count': 0,
            'md_count': 0,
            'txt_count': 0,
            'verilog_count': 0,
            'vhdl_count': 0,
            'total_chunks': 0,
            'total_tokens': 0,
            'failed_files': [],
            'processed_hashes': set(),  # Duplicate kontrolÃ¼
            'sources': {},  # Hangi kaynaktan kaÃ§ dÃ¶kÃ¼man
            'start_time': datetime.now()
        }
    
    def get_file_hash(self, filepath):
        """Dosya iÃ§eriÄŸinin hash'ini al (duplicate kontrolÃ¼ iÃ§in)"""
        try:
            with open(filepath, 'rb') as f:
                return hashlib.md5(f.read()).hexdigest()
        except:
            return None
    
    def count_tokens(self, text):
        """Token sayÄ±sÄ±"""
        return len(self.tokenizer.encode(text))
    
    def chunk_text(self, text, chunk_size=1000, overlap=200):
        """Metni akÄ±llÄ± chunk'lara bÃ¶l"""
        paragraphs = text.split('\n\n')
        chunks = []
        current_chunk = ""
        current_tokens = 0
        
        for para in paragraphs:
            para_tokens = self.count_tokens(para)
            
            if para_tokens > chunk_size:
                lines = para.split('\n')
                for line in lines:
                    line_tokens = self.count_tokens(line)
                    
                    if current_tokens + line_tokens > chunk_size:
                        if current_chunk:
                            chunks.append(current_chunk.strip())
                        current_chunk = line + '\n'
                        current_tokens = line_tokens
                    else:
                        current_chunk += line + '\n'
                        current_tokens += line_tokens
            else:
                if current_tokens + para_tokens > chunk_size:
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                    current_chunk = para + '\n\n'
                    current_tokens = para_tokens
                else:
                    current_chunk += para + '\n\n'
                    current_tokens += para_tokens
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def extract_pdf_text(self, pdf_path):
        """PDF'den text Ã§Ä±kar"""
        if not PDF_AVAILABLE:
            return None
        
        try:
            reader = PdfReader(pdf_path)
            text = ""
            for page in reader.pages:
                text += page.extract_text() + "\n"
            return text
        except Exception as e:
            print(f"   âŒ PDF okuma hatasÄ±: {e}")
            return None
    
    def find_all_documents(self):
        """TÃ¼m sistemi tara, dokÃ¼manlarÄ± bul"""
        all_files = {'pdf': [], 'md': [], 'txt': [], 'verilog': [], 'vhdl': []}
        
        print("\nğŸ” Sistem taranÄ±yor...")
        
        for search_path in self.SEARCH_PATHS:
            path = Path(search_path)
            if not path.exists():
                continue
            
            print(f"   ğŸ“ {search_path}")
            
            # PDF'ler
            pdfs = list(path.rglob("*.pdf"))
            all_files['pdf'].extend(pdfs)
            
            # Markdown
            mds = list(path.rglob("*.md"))
            all_files['md'].extend(mds)
            
            # Text
            txts = list(path.rglob("*.txt"))
            all_files['txt'].extend(txts)
            
            # Verilog/SystemVerilog (sadece templates ve examples)
            if "Vivado" in str(path):
                for ext in ["*.v", "*.vh", "*.sv"]:
                    v_files = path.rglob(ext)
                    # Sadece Ã¶rnek ve template dosyalarÄ±nÄ± al
                    all_files['verilog'].extend([f for f in v_files 
                        if any(x in str(f).lower() for x in ['template', 'example', 'demo', 'samples'])])
            
            # VHDL (sadece templates ve examples)
            if "Vivado" in str(path):
                vhdl_files = path.rglob("*.vhd")
                all_files['vhdl'].extend([f for f in vhdl_files 
                    if any(x in str(f).lower() for x in ['template', 'example', 'demo', 'samples'])])
        
        return all_files
    
    def add_document_to_db(self, text, metadata):
        """DokÃ¼manÄ± ChromaDB'ye ekle (hata toleranslÄ±)"""
        chunks = self.chunk_text(text)
        
        for i, chunk in enumerate(chunks):
            doc_id = f"{metadata['source']}_{metadata['filename']}_{i}"
            chunk_metadata = {
                **metadata,
                'chunk_index': i,
                'total_chunks': len(chunks),
                'tokens': self.count_tokens(chunk)
            }
            
            try:
                self.collection.add(
                    documents=[chunk],
                    metadatas=[chunk_metadata],
                    ids=[doc_id]
                )
                
                self.stats['total_chunks'] += 1
                self.stats['total_tokens'] += chunk_metadata['tokens']
            except Exception as e:
                print(f"\n   âš ï¸ Chunk {i} eklenemedi: {str(e)[:100]}")
                self.stats['failed_files'].append(f"{metadata['filename']}_chunk{i}")
                # Ä°nternet hatasÄ± varsa bekle ve tekrar dene
                if "Connection" in str(e) or "timeout" in str(e).lower():
                    import time
                    print("   â³ 5 saniye bekleniyor...")
                    time.sleep(5)
                    try:
                        self.collection.add(
                            documents=[chunk],
                            metadatas=[chunk_metadata],
                            ids=[doc_id]
                        )
                        self.stats['total_chunks'] += 1
                        self.stats['total_tokens'] += chunk_metadata['tokens']
                    except:
                        pass
        
        return len(chunks)
    
    def process_pdf(self, pdf_path, source_name):
        """PDF iÅŸle"""
        file_hash = self.get_file_hash(pdf_path)
        if file_hash in self.stats['processed_hashes']:
            return 0  # Duplicate, skip
        
        text = self.extract_pdf_text(pdf_path)
        if not text or len(text.strip()) < 100:
            return 0
        
        metadata = {
            'source': source_name,
            'filename': pdf_path.name,
            'filepath': str(pdf_path),
            'type': 'pdf',
            'hash': file_hash
        }
        
        chunks = self.add_document_to_db(text, metadata)
        self.stats['processed_hashes'].add(file_hash)
        self.stats['pdf_count'] += 1
        
        return chunks
    
    def process_markdown(self, md_path, source_name):
        """Markdown iÅŸle"""
        try:
            with open(md_path, 'r', encoding='utf-8') as f:
                text = f.read()
            
            file_hash = hashlib.md5(text.encode()).hexdigest()
            if file_hash in self.stats['processed_hashes']:
                return 0
            
            metadata = {
                'source': source_name,
                'filename': md_path.name,
                'filepath': str(md_path),
                'type': 'markdown',
                'hash': file_hash
            }
            
            chunks = self.add_document_to_db(text, metadata)
            self.stats['processed_hashes'].add(file_hash)
            self.stats['md_count'] += 1
            
            return chunks
        except:
            return 0
    
    def process_verilog(self, v_path, source_name):
        """Verilog/SystemVerilog iÅŸle"""
        try:
            with open(v_path, 'r', encoding='utf-8', errors='ignore') as f:
                text = f.read()
            
            file_hash = hashlib.md5(text.encode()).hexdigest()
            if file_hash in self.stats['processed_hashes']:
                return 0
            
            metadata = {
                'source': source_name,
                'filename': v_path.name,
                'filepath': str(v_path),
                'type': 'verilog',
                'hash': file_hash
            }
            
            chunks = self.add_document_to_db(text, metadata)
            self.stats['processed_hashes'].add(file_hash)
            self.stats['verilog_count'] += 1
            
            return chunks
        except:
            return 0
    
    def process_vhdl(self, vhd_path, source_name):
        """VHDL iÅŸle"""
        try:
            with open(vhd_path, 'r', encoding='utf-8', errors='ignore') as f:
                text = f.read()
            
            file_hash = hashlib.md5(text.encode()).hexdigest()
            if file_hash in self.stats['processed_hashes']:
                return 0
            
            metadata = {
                'source': source_name,
                'filename': vhd_path.name,
                'filepath': str(vhd_path),
                'type': 'vhdl',
                'hash': file_hash
            }
            
            chunks = self.add_document_to_db(text, metadata)
            self.stats['processed_hashes'].add(file_hash)
            self.stats['vhdl_count'] += 1
            
            return chunks
        except:
            return 0
    
    def train_full_system(self):
        """TÃ¼m sistemi tara ve eÄŸit"""
        print("=" * 80)
        print("ğŸš€ TAM SÄ°STEM RAG EÄÄ°TÄ°MÄ°")
        print("=" * 80)
        
        # DokÃ¼manlarÄ± bul
        all_files = self.find_all_documents()
        
        total_files = (len(all_files['pdf']) + len(all_files['md']) + 
                      len(all_files['txt']) + len(all_files['verilog']) + len(all_files['vhdl']))
        print(f"\nğŸ“Š Toplam Dosya: {total_files}")
        print(f"   ğŸ“„ PDF: {len(all_files['pdf'])}")
        print(f"   ğŸ“ Markdown: {len(all_files['md'])}")
        print(f"   ğŸ“„ Text: {len(all_files['txt'])}")
        print(f"   ğŸ”§ Verilog: {len(all_files['verilog'])}")
        print(f"   ğŸ”§ VHDL: {len(all_files['vhdl'])}")
        
        # PDF'leri iÅŸle
        if all_files['pdf']:
            print("\n" + "-" * 80)
            print("ğŸ“„ PDF DOSYALARI Ä°ÅLENÄ°YOR")
            print("-" * 80)
            
            for pdf_path in tqdm(all_files['pdf'], desc="PDF'ler"):
                # KaynaÄŸÄ± belirle
                source = "unknown"
                path_str = str(pdf_path)
                if "Xilinx" in path_str:
                    if "Vivado" in path_str:
                        source = "xilinx_vivado"
                    elif "Vitis" in path_str:
                        source = "xilinx_vitis"
                    elif "data" in path_str:
                        source = "xilinx_data"
                elif "docs_fpga" in path_str:
                    source = "docs_fpga"
                elif "ax7010" in path_str:
                    source = "ax7010_project"
                elif "z7lite" in path_str:
                    source = "z7lite_project"
                elif "z7nano" in path_str:
                    source = "z7nano_project"
                elif "VIVADO_DOCS" in path_str:
                    source = "vivado_docs_workspace"
                
                chunks = self.process_pdf(pdf_path, source)
                
                # Ä°statistik gÃ¼ncelle
                if source not in self.stats['sources']:
                    self.stats['sources'][source] = 0
                self.stats['sources'][source] += 1
        
        # Markdown'larÄ± iÅŸle
        if all_files['md']:
            print("\n" + "-" * 80)
            print("ğŸ“ MARKDOWN DOSYALARI Ä°ÅLENÄ°YOR")
            print("-" * 80)
            
            for md_path in tqdm(all_files['md'], desc="Markdown'lar"):
                source = "markdown_docs"
                if "VIVADO_DOCS" in str(md_path):
                    source = "vivado_docs_workspace"
                
                chunks = self.process_markdown(md_path, source)
                
                if source not in self.stats['sources']:
                    self.stats['sources'][source] = 0
                if chunks > 0:
                    self.stats['sources'][source] += 1
        
        # Verilog dosyalarÄ±nÄ± iÅŸle
        if all_files['verilog']:
            print("\n" + "-" * 80)
            print("ğŸ”§ VERILOG TEMPLATE VE Ã–RNEKLER Ä°ÅLENÄ°YOR")
            print("-" * 80)
            
            for v_path in tqdm(all_files['verilog'], desc="Verilog"):
                source = "vivado_verilog_templates"
                chunks = self.process_verilog(v_path, source)
                
                if source not in self.stats['sources']:
                    self.stats['sources'][source] = 0
                if chunks > 0:
                    self.stats['sources'][source] += 1
        
        # VHDL dosyalarÄ±nÄ± iÅŸle
        if all_files['vhdl']:
            print("\n" + "-" * 80)
            print("ğŸ”§ VHDL TEMPLATE VE Ã–RNEKLER Ä°ÅLENÄ°YOR")
            print("-" * 80)
            
            for vhd_path in tqdm(all_files['vhdl'], desc="VHDL"):
                source = "vivado_vhdl_templates"
                chunks = self.process_vhdl(vhd_path, source)
                
                if source not in self.stats['sources']:
                    self.stats['sources'][source] = 0
                if chunks > 0:
                    self.stats['sources'][source] += 1
        
        # SonuÃ§larÄ± gÃ¶ster
        self.show_results()
    
    def show_results(self):
        """EÄŸitim sonuÃ§larÄ±nÄ± gÃ¶ster"""
        duration = (datetime.now() - self.stats['start_time']).total_seconds()
        embedding_cost = (self.stats['total_tokens'] / 1000) * 0.0001
        
        print("\n" + "=" * 80)
        print("âœ… TAM SÄ°STEM RAG EÄÄ°TÄ°MÄ° TAMAMLANDI")
        print("=" * 80)
        
        print(f"\nğŸ“Š Ä°statistikler:")
        print(f"   â±ï¸  SÃ¼re: {duration:.1f} saniye")
        print(f"   ğŸ“„ PDF: {self.stats['pdf_count']} dosya")
        print(f"   ğŸ“ Markdown: {self.stats['md_count']} dosya")
        print(f"   ï¿½ Verilog: {self.stats['verilog_count']} dosya")
        print(f"   ğŸ”§ VHDL: {self.stats['vhdl_count']} dosya")
        print(f"   ï¿½ğŸ“š Toplam Chunk: {self.stats['total_chunks']}")
        print(f"   ğŸ”¢ Toplam Token: {self.stats['total_tokens']:,}")
        print(f"   ğŸ’¾ Database: {self.collection.count()} dÃ¶kÃ¼man")
        print(f"   ğŸ’° Tahmini Maliyet: ${embedding_cost:.4f}")
        
        print(f"\nğŸ“‚ Kaynaklar:")
        for source, count in sorted(self.stats['sources'].items()):
            print(f"   â€¢ {source}: {count} dosya")
        
        if self.stats['failed_files']:
            print(f"\nâš ï¸ BaÅŸarÄ±sÄ±z: {len(self.stats['failed_files'])} dosya")
        
        print("=" * 80)
        
        # Ä°statistikleri kaydet
        stats_file = Path(self.db_path) / "full_training_stats.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump({
                'duration_seconds': duration,
                'pdf_count': self.stats['pdf_count'],
                'md_count': self.stats['md_count'],
                'verilog_count': self.stats['verilog_count'],
                'vhdl_count': self.stats['vhdl_count'],
                'total_chunks': self.stats['total_chunks'],
                'total_tokens': self.stats['total_tokens'],
                'embedding_cost': embedding_cost,
                'sources': self.stats['sources'],
                'timestamp': datetime.now().isoformat()
            }, f, indent=2)
        
        print(f"\nğŸ“ Ä°statistikler kaydedildi: {stats_file}")
        print("\nâœ¨ ArtÄ±k 'python vivado_agent.py' ile asistanÄ± kullanabilirsiniz!")


def main():
    """Ana fonksiyon"""
    try:
        trainer = FullSystemRAGTrainer()
        trainer.train_full_system()
    except KeyboardInterrupt:
        print("\n\nâš ï¸ KullanÄ±cÄ± tarafÄ±ndan iptal edildi")
    except Exception as e:
        print(f"\nâŒ HATA: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
